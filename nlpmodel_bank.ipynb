{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f4b29-f13f-42eb-9cda-1515c7968336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import sklearn \n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install nltk\n",
    "\n",
    "filepath=r'C:\\Users\\Admin\\OneDrive\\Desktop\\dsai-nlp\\complaints.csv'\n",
    "f1=r'C:\\Users\\Admin\\OneDrive\\Desktop\\dsai-nlp\\final_dataframe.csv'\n",
    "df=pd.read_csv(filepath)\n",
    "df2=pd.read_csv(f1)\n",
    "\n",
    "#Handling missing values\n",
    "impute=SimpleImputer(strategy='most_frequent')\n",
    "df=pd.DataFrame(impute.fit_transform(df),columns=df.columns)\n",
    "                     \n",
    "\n",
    "#data preparation\n",
    "x=df['narrative'].values\n",
    "unique=df[\"product\"].unique()\n",
    "unique\n",
    "df_encoded = pd.get_dummies(df['product'])\n",
    "df = pd.concat([df, df_encoded], axis=1)\n",
    "\n",
    "y = np.argmax(df[['credit_card', 'credit_reporting', 'debt_collection', 'mortgages_and_loans', 'retail_banking']].values, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#data splitting\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=100)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "\n",
    "#train the RfR\n",
    "model=RandomForestClassifier()\n",
    "model.fit(x_train_vectorized,y_train)\n",
    "\n",
    "#Model evaluation\n",
    "#making predictions\n",
    "\n",
    "y_pred=model.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "#calculate metrics\n",
    "accuracy=accuracy_score(y_test,y_pred,average='weighted')\n",
    "precision=precision_score(y_test,y_pred,average='weighted')\n",
    "recall=recall_score(y_test,y_pred,average='weighted')\n",
    "f1=f1_score(y_test,y_pred,average='weighted')\n",
    "report=classification_report(y_test,y_pred,average='weighted')\n",
    "\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(f1)\n",
    "print(recall)\n",
    "print(report)\n",
    "\n",
    "\n",
    "#BERT\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "seq_len = [len(i.split()) for i in x_train]\n",
    "#inputs for BERT\n",
    "inputs = tokenizer(X.tolist(), return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "attention_mask = inputs['attention_mask']\n",
    "#label-->tensor\n",
    "labels_tensor = torch.tensor(y.tolist())\n",
    "\n",
    "#forward pass\n",
    "outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=labels_tensor)\n",
    "loss = outputs.loss  # Loss for backpropagation\n",
    "logits = outputs.logits  # Raw predictions\n",
    "\n",
    "inputs_test = tokenizer(x_test.tolist(), return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "with torch.no_grad():\n",
    "    outputs_test = bert_model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n",
    "    predictions = torch.argmax(outputs_test.logits, dim=-1)\n",
    "\n",
    "# Convert predictions to numpy array for sklearn metrics\n",
    "predictions = predictions.numpy()\n",
    "\n",
    "# Evaluate BERT Model\n",
    "print(\"\\nBERT Classifier Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Precision:\", precision_score(y_test, predictions, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, predictions, average='weighted'))\n",
    "print(\"F1 Score:\", f1_score(y_test, predictions, average='weighted'))\n",
    "print(classification_report(y_test, predictions, target_names=label_encoder.classes_))\n",
    "\n",
    "\n",
    "#FINE TUNING\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Define the parameter distributions for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'countvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'multinomialnb__alpha': uniform(0.1, 1.0),\n",
    "}\n",
    "\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "randomized_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "# Train the model with the best hyperparameters\n",
    "randomized_search.fit(X_train, y_train)\n",
    "# Get the best model from the randomized search\n",
    "best_model = randomized_search.best_estimator_\n",
    "# Predict on the test set\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "# Evaluate the best model\n",
    "accuracy = accuracy_score(y_test, y_pred_best)\n",
    "print('Best Model Accuracy',accuracy)\n",
    "\n",
    "print(\"Best Hyperparameters:\", randomized_search.best_params_)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_best))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f5daf4-4f8e-442f-9b6b-3ec49fbb9bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3c37af-f56f-489f-87f8-e5715cbd5804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
